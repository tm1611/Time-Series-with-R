---
title: "Forecasting Product Demand in R"
author: "Timo Meiendresch"
date: "17/05/2019"
knit: (function(input_file, encoding) {
  out_dir <- 'html_outputs';
  rmarkdown::render(input_file,
  encoding=encoding,
  output_file=file.path(dirname(input_file), out_dir, 'Forecasting Product Demand in R.html'))})
---

```{r, message=FALSE, echo=FALSE}
rm(list=ls())
graphics.off()
```

```{r, message=FALSE}
# libraries
library("ggplot2")
library("forecast")
library("xts")
```

# 1. Forecasting Product demand in R 

### Loading data into xts object

- two pieces: date and data matrix
- combine

```{r}
# load csv
x <- read.csv(url("https://assets.datacamp.com/production/course_6021/datasets/Bev.csv"))

dates <- seq(as.Date("2014-01-19"), length = 176, by = "weeks")
bev_xts <- xts(x, order.by = dates) 


# Create the individual region sales as their own objects
MET_hi <- bev_xts[,"MET.hi"]
MET_lo <- bev_xts[,"MET.lo"]
MET_sp <- bev_xts[,"MET.sp"]

# Sum the region sales together
MET_t <- MET_hi + MET_lo + MET_sp

# Plot the metropolitan region total sales
plot(MET_t)

```

### ARIMA time series 101

```{r}
# Split the data into training and validation
MET_t_train <- MET_t[index(MET_t) < "2017-01-01"]
MET_t_valid <- MET_t[index(MET_t) >= "2017-01-01"]

# Use auto.arima() function for metropolitan sales
MET_t_model <- auto.arima(MET_t_train)
```

### 2 Common measures of forecast accuracy
To assess how good/bad our prediction is we use measures of forecast accuracy, where we compare our prediction to actual, realized data. In order to have actual data we use a validation data set, where some realizations are held besides the training process. We have therefore 2 data sets, training and test. We never train using the test data!

- Mean Absolute Error (MAE)

$$\frac{1}{n} \sum_{i=1}^{n} |Y_t - \hat{Y}_t|$$

- Mean Absolute Percentage Error (MAPE)

$$\frac{1}{n} \sum_{i=1}^{n} \bigg| \frac{Y_t - \hat{Y_t}}{Y_t} \bigg| \times 100$$
MAPE to set the prediction error in relative terms. 

```{r}
# Forecast the first 22 weeks of 2017
forecast_MET_t <- forecast(MET_t_model, h = 22)

# Plot this forecast #
plot(forecast_MET_t)

# Convert to numeric for ease
for_MET_t <- as.numeric(forecast_MET_t$mean)
v_MET_t <- as.numeric(MET_t_valid)

# Calculate the MAE
MAE <- mean(abs(v_MET_t - for_MET_t))

# Calculate the MAPE
MAPE <- 100* mean(abs( (v_MET_t - for_MET_t)/v_MET_t ) )

# Print to see how good your forecast is!
print(MAE)
print(MAPE)

# Convert your forecast to an xts object
for_dates <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_t_xts <- xts(forecast_MET_t$mean, order.by = for_dates)

# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Plot the validation data set
plot(MET_t_valid, main = 'Forecast Comparison', ylim = c(4000, 8500))

# Overlay the forecast of 2017
lines(for_MET_t_xts, col = "blue")

# Convert the limits to xts objects
lower <- xts(forecast_MET_t$lower[,2], order.by = for_dates)
upper <- xts(forecast_MET_t$upper[,2], order.by = for_dates)

# Adding confidence intervals of forecast to plot
lines(lower, col = "blue", lty = "dashed")
lines(upper, col = "blue", lty = "dashed")

```

# Chapter 2: Components of Demand

### Price elasticity

- Price elasticity is the economic measure of how much demand "reacts to changes in price"
- As price  changes, it is expected that demand changes as well, but how much?

$$Price \, Elasticity = \frac{\Delta Demand}{\Delta Price}$$
- **Elastic:** Demand changes in percent are larger than percentage change in price (PE > 1)
- **Inelastic:** % change in demand smaller than the % change in price (PE < 1)
- **Unit elastic:** Percentage change in demand and price are the same (PE = 1)

```{r}
# Create training and testing data
bev_xts_train <- bev_xts[index(bev_xts) < "2017-01-01"]
bev_xts_valid <- bev_xts[index(bev_xts) >= "2017-01-01"]

# save prices
l_MET_hi_p <- as.vector(log(bev_xts_train[,"MET.hi.p"]))

# Save as a data frame
MET_hi_train <- data.frame(as.vector(log(MET_hi[index(MET_hi) < "2017-01-01"])), l_MET_hi_p)
colnames(MET_hi_train) <- c("log_sales", "log_price")

# Calculate the regression
model_MET_hi <- lm(log_sales ~ log_price, data = MET_hi_train)
```

Price elasticity is greater than 1 in absolute terms. Hence, high end product is elastic


### Seasonal / holiday / promotional effects
Linear regression may now help to evaluate other relationships. We'll add seasonal, holiday, and promotion effects to previous regression.

- Next: Create effects variable for valentine and check if valentine has a significant effect.

```{r}
# Creating effects
v_dates <- as.Date(c("2014-02-09", "2015-02-08", "2016-02-07"))
valentine <- as.xts(rep(1,3), order.by = v_dates)

```

```{r}
# Create date indices for New Year's week
n.dates <- as.Date(c("2014-12-28", "2015-12-27", "2016-12-25"))

# Create xts objects for New Year's
newyear <- as.xts(rep(1, 3), order.by = n.dates)

# Create sequence of dates for merging
dates_train <- seq(as.Date("2014-01-19"), length = 154, by = "weeks")

# Merge training dates into New Year's object
newyear <- merge(newyear, dates_train, fill = 0)

# add newyear to new df
MET_hi_train_2 <- data.frame(MET_hi_train, as.vector(newyear))
colnames(MET_hi_train_2)[3] <- "newyear"

model_MET_hi_full <- lm(log_sales ~ log_price + newyear, data = MET_hi_train_2)

summary(model_MET_hi_full)
```

### Forecasting with Regression

- How to "predict" future input variables?
- Holidays and promotions - as used above - are known in advance. 
- Prices may be complicated: Either known in advance (or assumed) or if not known we could use time series modeling like arima.

```{r}
# future input example
v.dates_v <- as.Date("2017-02-12")
valentine_v <- as.xts(1, order.by = v.dates_v)
dates_valid <- seq(as.Date("2017-01-01"), length=22, by="weeks")
valentine_v <- merge(valentine_v, dates_valid, fill=0)

# merge data
l_M_hi_p_valid <- log(bev_xts_valid[, "M.hi.p"])
model_M_valid <- data.frame(as.vector(l_M_hi_p_valid), as.vector(valentine_v))
colnames(model_M_valid) <- c("log_price", "valentine")

# 
#pred_M_hi <- predict(model_M_hi_full, model_M_valid)

```

```{r}
# Subset the validation prices
l_MET_hi_p_valid <- as.vector(log(bev_xts_valid[,"MET.hi.p"]))

# Create a validation data frame
MET_hi_valid <- data.frame(l_MET_hi_p_valid)
colnames(MET_hi_valid) <- "log_price"

# Predict the log of sales for your high end product
pred_MET_hi <- predict(model_MET_hi, MET_hi_valid)

# Convert predictions out of log scale
pred_MET_hi <- exp(pred_MET_hi)

# Convert to an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
pred_MET_hi_xts <- xts(pred_MET_hi, order.by = dates_valid)

# Plot the forecast
plot(pred_MET_hi_xts)

# Calculate and print the MAPE
MET_hi_v <- bev_xts_valid[,"MET.hi"]

MAPE <- 100*mean(abs((pred_MET_hi_xts - MET_hi_v)/MET_hi_v))
print(MAPE)
```

# Blending regression with time series
Combine regression and time series analysis. Objective is to reduce residuals further which can be done by:

- Adding more important variables to the model
- Use time series if residuals are related over time.

Use `residuals()` function to check for patterns.  

```{r}
# Calculate the residuals from the model
MET_hi_full_res <- residuals(model_MET_hi_full)

# Convert the residuals to an xts object
MET_hi_full_res <- xts(MET_hi_full_res, order.by = dates_train)

# Plot the histogram of the residuals
hist(MET_hi_full_res)

# Plot the residuals over time
plot(MET_hi_full_res)
```

Residuals seem to be related. We'll may want to use time series to solve this predict them. Squeeze the pattern out of the errors of the form

$$y = f(x_1, x_2,...) + e $$ 

**Next step:** Forecasting residuals using `auto.arima()`

```{r}
# Build an ARIMA model on the residuals: 
MET_hi_arima <- auto.arima(MET_hi_full_res)

# Look at a summary of the model
summary(MET_hi_arima)

# Forecast 22 weeks with residual model:
for_MET_hi_arima <- forecast(MET_hi_arima, h=22)

# Convert your forecasts into an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")

for_MET_hi_arima <- xts(for_MET_hi_arima$mean, order.by = dates_valid)

# Plot the forecast
plot(for_MET_hi_arima)
```

There are different ways to combine forecasting techniques. Two of them are using transfer functions and Ensembling.

- Transfer functions: Everything gets built into one model
- Ensembling: Averaging of multiple types of model forecasts

Transfer functions use inputs (as in regression) and the errors are modeled using time series (i.e. arima). 

- Combining two different techniques into one mathematically:

$$ log(Y_t) = \beta_0 + \beta_1 log(X_t) + \beta_2 X_2 + ... + \epsilon_t $$

$$ \epsilon_t = \alpha_0 + \alpha_1 \epsilon_{t-1} + \alpha_2 \epsilon_{t-2} + ... + \epsilon $$

- Combining the forecasts into one mathematically:

$$
log(Y_t) = log(\hat{Y}_t) + \hat{\epsilon}_t
$$

$$
Y_t = \hat{Y}_t \times exp(\hat{\epsilon}_t)
$$

Ensembling combines the forecast itself. There are a variety of ways of blending the forecasts together. One easy way is to take the average of the forecasts.

```{r}
# Convert your residual forecast to the exponential version
for_MET_hi_arima <- exp(for_MET_hi_arima)

# Multiply your forecasts together!
for_MET_hi_final <- pred_MET_hi_xts * for_MET_hi_arima

# Plot the final forecast - don't touch the options!
plot(for_MET_hi_final, ylim = c(1000, 4300))

# Overlay the validation data set
lines(MET_hi_v, col = "blue")

# Calculate the MAE
MAE <- mean(abs(for_MET_hi_final - MET_hi_v))
print(MAE)

# Calculate the MAPE
MAPE <- 100*mean(abs((for_MET_hi_final - MET_hi_v)/MET_hi_v))
print(MAPE)

# Build an ARIMA model using the auto.arima function
MET_hi_model_arima <- auto.arima(MET_hi)

# Forecast the ARIMA model
for_MET_hi <- forecast(MET_hi_model_arima, h = 22)

# Save the forecast as an xts object
dates_valid <- seq(as.Date("2017-01-01"), length = 22, by = "weeks")
for_MET_hi_xts <- xts(for_MET_hi$mean, order.by = dates_valid)

# Calculate the MAPE of the forecast
MAPE <- 100*mean(abs((MET_hi_v - for_MET_hi_xts)/MET_hi_v))
print(MAPE)

# Ensemble the two forecasts together
for_MET_hi_en <- 0.5*(for_MET_hi_xts + pred_MET_hi_xts)

# Calculate the MAE and MAPE
MAE <- mean(abs((for_MET_hi_en - MET_hi_v)))
print(MAE)

MAPE <- 100*mean(abs((for_MET_hi_en - MET_hi_v)/MET_hi_v))
print(MAPE)
``` 

# Bottom-Up Hierarchical Forecasting

- Hierarchical forecasting can be used when different items that need to be forecasted can be arranged in a logical hierarchy.
- Forecasts need to be reconciled up and down the hierarchy

Three types of hierarchial forcasting:
1. Bottom-up Forecasting
2. Top-down Forecasting
3. Middle-out Forecasting

### Bottom-up Hierarchical Forecasting

- Calculate individual forecasts for the smallest unit (i.e. arima, regression, ensemble) and add them together to get the next bigger unit. 

```{r}
# own mape function
mape <- function(yhat, y){
  mean(abs((y - yhat) / y))*100
}

# Build a time series model 
MET_sp_model_arima <- auto.arima(MET_sp)

# Forecast the time series model for 22 periods
for_MET_sp <- forecast(MET_sp_model_arima, 22)

# Create an xts object
for_MET_sp_xts <- xts(for_MET_sp$mean, order.by=dates_valid)

# Calculate the MAPE
#MAPE <- mape(for_MET_sp_xts, MET_sp_v)
#print(MAPE)
```

Next steps (not done in R due different data processing): 

- Ensemble a regression model prediction and a time series forecast together
- Adding up all individual forecasts to get the regional sales. Check MAPE using validation data

### Top-Down Hierarchical Forecasting
Forecast starts at the top of the hierarchy and then goes down (reconciled down) to smaller units.

Two techniques:

1. Average of historical proportions
2. Proportion of historical averages

Reconciled forecasts at lower level not as accurate as directly forecasting. 

To do in R (data not available locally):

- Build a regional model and calculate forecasts
- Calculate MAPE and print the forecast
- Calculate average proportions
- Disttribute out forecast to each product
- check MAPE

Proportion of historical averages:

- take the mean of each product in the past and divide by the mean of the total products to get to the proportions
- distirbute out your foreacast again to each product

### Middle-Out Hierarchical Forecasting
Start in the middle and then go up as well as  down to distribute this initial forecast.

```{r}
SEC_t_v <- bev_xts$SEC.hi + bev_xts$SEC.lo
SEC_total <- window(SEC_t_v, end = "2016-12-25")

# Build a time series model for the region
SEC_t_model_arima <- auto.arima(SEC_total)

# Forecast the time series model
for_SEC_t <- forecast(SEC_t_model_arima, 22)

# Make into an xts object
for_SEC_t_xts <- xts(for_SEC_t$mean, order.by=dates_valid)

# Calculate the MAPE
MAPE <- mape(for_SEC_t_xts, SEC_t_v)
print(MAPE)




```

### Summary:

- Chapter 1: Using time series to forecast demand
- Chapter 2: Incorporating external factors in demand forecast
- Chapter 3: Blending time sereis and regression approaches
- Chapter 4: Hierarchical forecasting

Possible extensions: 

- Incorporate more exteranl factors to deman (cross-elasticities)



